{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LunarLander Experiments - random",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR7sTUMPbuB7"
      },
      "source": [
        "Todo:\n",
        "- Get the video workings\n",
        "- Implenting the new library\n",
        "- Saving/ Loading Models\n",
        "- Dataloader for levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIZMR-nLLFFb"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es-gxjQx9Mmz",
        "outputId": "45681127-5db2-41c2-8457-cab921bb5d1a"
      },
      "source": [
        "!rm -r interaction_icaros\n",
        "!git clone --recursive https://github.com/jignesh284/interaction_icaros.git\n",
        "!pip install ./interaction_icaros/gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'interaction_icaros'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 7645 (delta 17), reused 33 (delta 6), pack-reused 7601\u001b[K\n",
            "Receiving objects: 100% (7645/7645), 109.81 MiB | 21.57 MiB/s, done.\n",
            "Resolving deltas: 100% (961/961), done.\n",
            "Submodule 'gym' (https://github.com/ndennler/gym.git) registered for path 'gym'\n",
            "Cloning into '/content/interaction_icaros/gym'...\n",
            "remote: Enumerating objects: 10840, done.        \n",
            "remote: Total 10840 (delta 0), reused 0 (delta 0), pack-reused 10840        \n",
            "Receiving objects: 100% (10840/10840), 4.14 MiB | 19.55 MiB/s, done.\n",
            "Resolving deltas: 100% (7342/7342), done.\n",
            "Submodule path 'gym': checked out '9da49acfed9745ec035693ca75ffefca56f56d90'\n",
            "Processing ./interaction_icaros/gym\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.17.3) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.17.3) (1.18.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: Pillow<=7.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.17.3) (7.0.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.17.3) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-cp36-none-any.whl size=1657737 sha256=77d9bd7a854e09d0f29790eac670a1fd7208667074a0c3d2813cd6ff582ab9af\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-co7jbxxd/wheels/18/90/2d/2584e83cf0777cfdd9e6cd60b4fa5559ad5340151bcf5930b6\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgsXNSe7sJMH"
      },
      "source": [
        "# ! git clone https://github.com/openai/baselines.git\n",
        "# ! pip install ./baselines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qeiquzEKat8",
        "outputId": "8ae34684-8255-4302-eca4-e06412e827c3"
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "\n",
        "# !pip install gym\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install pybullet > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg swig> /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup> /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install box2d 2>&1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (50.3.2)\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.6/dist-packages (2.3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynz7csYmKt7u",
        "outputId": "fb4b88f3-6e6f-4a9d-f9d7-d28b5fc75356"
      },
      "source": [
        "#environment imports\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "from gym import wrappers\n",
        "# gymlogger.set_level(40) #error only\n",
        "\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "#math imports\n",
        "import numpy as np\n",
        "import random\n",
        "import base64\n",
        "import math\n",
        "import time\n",
        "\n",
        "#plotting imports\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#I/O imports\n",
        "import glob\n",
        "import io\n",
        "\n",
        "#IPython Display schtuff\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f981ac6bc88>"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuV-9i7ZK8ki"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env, log_dir):\n",
        "  env = Monitor(env, log_dir, force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1YrXTk-LBSI"
      },
      "source": [
        "\n",
        "## Test Loading the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0v9-Ud1K_92",
        "outputId": "d72ab581-b164-4e70-8652-49106da17ee3"
      },
      "source": [
        "env = wrap_env(gym.make(\"LunarLander-v2\"), './video')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_x 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5KBxl_bLTuG",
        "outputId": "736c3ed5-a5d7-4250-9c7d-8a0fe0f3f676"
      },
      "source": [
        "#check out the Lunar Lander action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "n13wSwujLWFK",
        "outputId": "f9535321-10fd-4755-ece7-b62c0ae5e2eb"
      },
      "source": [
        "from IPython.display import HTML\n",
        "import base64\n",
        "import io\n",
        "\n",
        "env.load_terrain([7.0925,  5.8748,  5.7912,  5.2321,  3.1553,  3.1261,  3.1880,  6.9503,\n",
        "          7.7113,  5.8987,  5.6749])\n",
        "observation = env.reset()\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.show()\n",
        "# print(env.terrain_y_values)\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = 2 \n",
        "    observation, reward, done, info = env.step(action) \n",
        "    \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_x 10.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAAAhtZGF0AAAA1m1vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAAAAAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU3LjgzLjEwMA==\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "PHdzUko3Xe-v",
        "outputId": "39e7b023-f01c-4870-9d9b-bd051563bcf8"
      },
      "source": [
        "################# TODO: YOUR CODE BELOW #############################\n",
        "#################  Each todo here is 1 line of code  ################\n",
        "\n",
        "# 1. Make a new environment MiniGrid-Empty-8x8-v0\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "# 2. Reset the environment\n",
        "env.reset()\n",
        "\n",
        "# 3. Select the action right\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# 4. Take a step in the environment and store it in appropriate variables\n",
        "obs, reward, done, info = env.step(action)\n",
        "\n",
        "# 5. Render the current state of the environment\n",
        "img = env.render('rgb_array')\n",
        "################# YOUR CODE ENDS HERE ###############################\n",
        "\n",
        "print('Observation:', obs)\n",
        "print('Reward:', reward)\n",
        "print('Done:', done)\n",
        "print('Info:', info)\n",
        "print('Image shape:', img.shape)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_x 10.0\n",
            "initial_x 10.0\n",
            "Observation: [-0.00925226  1.398803   -0.46793166 -0.28211597  0.01061162  0.10490228\n",
            "  0.          0.        ]\n",
            "Reward: -1.1157205126488066\n",
            "Done: False\n",
            "Info: {}\n",
            "Image shape: (400, 600, 3)\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeeElEQVR4nO3de3RU5b3/8fc3FxJIuCdACEFAYpFDEAgICEWKQqHUwrEUwSNgQeC0Xuqp1Ypd/ZWu2qWnov5+yqlKjwpaUamiUuuNIlUrFQRFLiISbwVWuIncRCGX7++P2aEjArnNMNnJ57XWXtn72Xtmf58wfLLz5JnZ5u6IiEh4JCW6ABERqR4Ft4hIyCi4RURCRsEtIhIyCm4RkZBRcIuIhEzcgtvMRprZZjMrMrMb43UeEZGGxuIxj9vMkoH3geHANuBNYKK7vxvzk4mINDDxuuI+Fyhy9w/d/SjwGDAmTucSEWlQUuL0vLnA1qjtbUD/kx1sZnr7psRMWlomSUkpHD36OWVlJTRu3JzMxm1ISWpcq+ctLf+Sz7/czeHDn5GSkk56eibl5eUcPrw3RpWLfJW724na4xXclTKzGcCMRJ1f6qesrM6cf95PaNnkDIp2vMyKFfczaNAVFOZNpXFqy1o9967P32XV5j+wZctrfHPQTDq1GkLxgbW8suIu9uz5KEY9EKlcvIZKtgN5UdsdgrZj3H2eu/d1975xqkEaoDPPHExu8z5kNsqhpOQL4vE3nEOHdvPZvq0kWyPaNT2Hzp3PIzk5NebnETmZeAX3m0C+mXU2s0bABGBJnM4lAkB2dlc6tC0ks1EOxQfe4sMPV1BWVhrz85SVlbD5/eX8c+8KmqW1p2eXi8nPPz/m5xE5mbgEt7uXAlcBLwKbgEXuvjEe5xIBMEuic+cBtG/Wm8Mln7Jt91sUF2+K2/n27PmA9z5cyq7PN5LVpBud8vrTpEmruJ1PJFrcxrjd/TnguXg9v0i0rKwu5LXrS7O0XIo+fYmioteAfw2TlPlRSsu/rNU5yv2rV+8ffvgPOp1xLm0yetC+VR/y8nqxefPLtTqHSFUk7I+TIrGSlJRCp07nktPsHD4v2cO23W+za9cWANzL+PTTT3gvbQlGEtFhfnJ2wuPKg+eqcOTIQT76+A1ymvciq8k36JTXn61b3+bw4c9i0zGRk1BwS+i1atWRjjn9aJF+Bls+fZGioldwLwfAvZx1657hvfeW1vo87pGwjvbRRyvpfMYA2mb0oEXTPHJzC9iy5dVan0vkVBTcEnrNmrWlzI9SfHAt23au+drUvLKyEr74Yn9czn3kyCE2bnqO9EbN2PPZhxw4sCsu5xGJFpe3vFe7CL0BR2rBLInWrTvRtes3KSr6O3v2fHDaa2jZsgMHD+6mtPTIaT+31F8newOOgltEpI46WXDrY11FREJGwS0iEjIKbhGRkFFwi4iEjIJbRCRkFNwiIiGj4BYRCRkFt4hIyCi4RURCRsEtIhIyCm4RkZBRcIuIhIyCW0QkZGr1edxm9jFwECgDSt29r5m1Ah4HOgEfA+PdXbcEERGJkVhccX/L3Xu5e99g+0ZgmbvnA8uCbRERiZF4DJWMARYE6wuAsXE4h4hIg1Xb4HbgJTNbY2Yzgra27l4crO8A2tbyHCIiEqW295wc7O7bzawNsNTM3ove6e5+srvbBEE/40T7RETk5GJ26zIzmw0cAqYDQ9292MxygL+5+zcqeaxuXSYicpyY37rMzDLMrGnFOjAC2AAsAaYEh00BnqnpOURE5OtqfMVtZl2Ap4LNFGChu//WzFoDi4COwCdEpgPureS5dMUtInIc3eVdRCRkdJd3EZF6QsEtIhIyCm4RkZBRcIuIhIyCW0QkZBTcIiIho+AWEQkZBbeISMgouEVEQkbBLSISMgpuEZGQUXCLiISMgltEJGQU3CIiIaPgFhEJGQW3iEjIKLhFREJGwS0iEjIKbhGRkKk0uM3sATPbZWYbotpamdlSM9sSfG0ZtJuZ3WVmRWa2zsz6xLN4EZGGqCpX3POBkce13Qgsc/d8YFmwDTAKyA+WGcA9sSlTREQqVBrc7v4qsPe45jHAgmB9ATA2qv0hj3gDaGFmObEqVkREaj7G3dbdi4P1HUDbYD0X2Bp13Lag7WvMbIaZrTaz1TWsQUSkQUqp7RO4u5uZ1+Bx84B5ADV5vIhIQ1XTK+6dFUMgwdddQft2IC/quA5Bm4iIxEhNg3sJMCVYnwI8E9U+OZhdMgDYHzWkIiIiMWDupx6lMLNHgaFAFrAT+BXwNLAI6Ah8Aox3971mZsBcIrNQDgM/dPdKx7A1VCIi8nXubidqrzS4TwcFt4jI150suPXOSRGRkFFwi4iEjIJbRCRkFNwiIiGj4BYRCRkFt4hIyCi4RURCRsEtIhIyCm4RkZBRcIuIhIyCW0QkZBTcIiIho+AWEQkZBbeISMgouEVEQkbBLSISMgpuEZGQUXCLiIRMpcFtZg+Y2S4z2xDVNtvMtpvZ2mD5TtS+WWZWZGabzezb8SpcRKShqsrNgocAh4CH3L1H0DYbOOTuc447tjvwKHAu0B74K3CWu5dVcg7dc1JE5Dg1vueku78K7K3iecYAj7n7EXf/CCgiEuIiIhIjtRnjvsrM1gVDKS2Dtlxga9Qx24K2rzGzGWa22sxW16IGEZEGp6bBfQ9wJtALKAZur+4TuPs8d+/r7n1rWIOISINUo+B2953uXubu5cAf+NdwyHYgL+rQDkGbiIjESI2C28xyojb/HaiYcbIEmGBmaWbWGcgHVtWuRBERiZZS2QFm9igwFMgys23Ar4ChZtYLcOBjYCaAu280s0XAu0ApcGVlM0pERKR6Kp0OeFqK0HRAEZGvqfF0QBERqVsU3CIiIaPgFhEJGQW3iEjIKLhFREJGwS0iEjIKbhGRkFFwi4iEjIJbRCRkFNwiIiGj4BYRCRkFt4hIyCi4RURCRsEtIhIyCm4RkZBRcIuIhIyCW0QkZBTcIiIhU2lwm1memS03s3fNbKOZ/SRob2VmS81sS/C1ZdBuZnaXmRWZ2Toz6xPvToiINCRVueIuBa5z9+7AAOBKM+sO3Agsc/d8YFmwDTCKyN3d84EZwD0xr1pEpAGrNLjdvdjd3wrWDwKbgFxgDLAgOGwBMDZYHwM85BFvAC3MLCfmlYuINFDVGuM2s05Ab2Al0Nbdi4NdO4C2wXousDXqYduCtuOfa4aZrTaz1dWsWUSkQatycJtZJvAkcK27H4je5+4OeHVO7O7z3L2vu/etzuNERBq6KgW3maUSCe1H3H1x0LyzYggk+LoraN8O5EU9vEPQJiIiMZBS2QFmZsD9wCZ3vyNq1xJgCnBr8PWZqParzOwxoD+wP2pIpc7Kz88nMzMTgPLyct59911KSkoSXJWIyNdZZJTjFAeYDQZeA9YD5UHzTUTGuRcBHYFPgPHuvjcI+rnASOAw8EN3P+U4tplVa5ilJjIzM8nKyjq2fdFFFzFw4MBj28OGDaNNmzYAlJWVMXv2bH73u98pvEUkYdzdTtReaXCfDrEI7iZNmpCWlnZse9KkSZxxxhnHtrt168aFF154bDs5OZnk5OSTPl9ZWRkPP/wwc+bMYePGjbUtT0Sk2kIf3KmpqUQu5iOhO3nyZJo2bXps/4gRI+jXr9+x7YyMDFJTU2td23vvvcfFF1/M5s2bKS8vr/wBIiIxErrgLiwspFu3bse2p02bRpcuXSqOp3379qSkVDpEHxO7d+9m/vz5/OIXv9DQiYicNnU6uJs3b+5PP/00LVu2PNaWm5tLdnZ2Aqv6qpKSEtatW8dVV13FG2+8kehyRKQeKygo4JNPPmH//v0nDG7cPeFLYWGhh8XWrVt91KhRnpGRUTF3XYsWLVpqtZiZ9+jRwy+55BJfunSpFxcXe5CLJ8zM0zPWUI/k5ubyl7/8hQcffJCrr76aw4cPJ7okEQmhRo0akZuby8iRIzn//PMZPXo0GRkZx/6WdyoK7mqq+KZefvnltG7dmrvvvptly5YluCoRCYP09HSaNGnCFVdcQUFBAePHj690htuJKLhrKCkpiTFjxnDeeedx6aWX8re//Y3S0tJElyUidUxqairNmjVjypQpDBw4kAsvvJDMzMxaTa5QcNdSdnY2TzzxBEuXLuWKK65g//79iS5JROqATp06MXjwYC6//HK6detG+/btqzQMUhUK7hho3rw53//+9+nRowezZ8/m8ccfT3RJIpIABQUFdOnShZ///Oe0b9/+K28CjCUFd4yYGd26deO+++4jLS2N5557jj179iS6LBGJs4KCAs4++2xmzpzJ2WefTU5O/G8/oOCOsebNmzN//nyWL1/OxIkT2bVrV+UPEpHQSE1NpUOHDowcOZKhQ4cyatQoMjMzYzYMUhUK7jgwM771rW/x1FNPce+99/LII4/o7fIiIZaWlkZGRgbTpk2joKCASy65pEazQWJFwR0nZsZ5551Hnz59aNy4MQ8++KDeLi8SItGzQfr378+3v/1tMjIyTttHbZxK4iuo59LT07nzzjuZNGkSl156KVu3bq38QSKSEGZGx44dGTJkCJMnT6Z79+7k5OSc1mGQqlBwnwZNmjRh8ODBvPrqq9x7773cdtttGjoRqUPS0tIYPnw4s2bNIjc3N26zQWJFwX0aderUiV//+tckJyfzyCOP8MknnyS6JJEGLTMzk/PPP59Zs2YxYMCAhI1ZV1e17vIutZeWlsbNN9/M0qVL6dGjR6LLEWmwOnbsyNy5c/nzn//MoEGDQhPaoOBOCDOja9euLF68mJ/97Gc0atQo0SWJNAhJSUnk5ORw6623smbNGiZPnlznxq+rotLgNrM8M1tuZu+a2UYz+0nQPtvMtpvZ2mD5TtRjZplZkZltNrNvx7MDYWVm5Ofnc8stt/Cb3/yG9PT0RJckUm9V3Hxl9uzZrF+/nuuvv56srKxQhjZApZ+VDeQAfYL1psD7QHdgNvCzExzfHXgHSAM6Ax8Ayac6R5g+jzsejh496hs3bvTCwsKEfy6wFi31bTEz/9GPfuTFxcVeXl6e6P/uVXaqz+Ou9Irb3Yvd/a1g/SCwCcg9xUPGAI+5+xF3/wgoAs6t7DwNWWpqKt27d2fx4sWMGzeOJk2aJLokkdBLTk5m+PDh/PWvf2XOnDm0a9cuvFfYx6nWGLeZdQJ6AyuDpqvMbJ2ZPWBmFfcdywWiJytv49RBL4GOHTuyaNEi7rrrrq/cb1NOrUWLFjz88ESuuQb+7d8iS9R9pAWYMeMy7r03kwEDIt+f4Pat9VJ6ejrDhg3jxRdf5Nlnn2XYsGH17mKoytMBzSwTeBK41t0PmNk9wG+I/DryG+B2YGo1nm8GMAMigSURZsbUqVP53ve+x2WXXcZLL72U6JLqvNTUVM49tyPdusGkSZG29eth1y4oLYWHH4bPP4ejRyNtDVGXLmfQv38KhYWR7YMHYdWqyPrbb8Prr0fWd++GI0cSU2MsDBo0iF/+8peMGDECoN5cYR+vSsFtZqlEQvsRd18M4O47o/b/AXg22NwO5EU9vEPQ9hXuPg+YB9C3b1+vSfH1lZmRnZ3NH//4R+6//35uueUWDhw4kOiy6rzo/6M9e0a+usPw4ZGve/fC8uWR9hUr/hVWDUnF96hZM7jwwsj6sGHwX/8VWX/ttcgPty+/hHnzwhPiWVlZXHnllUyfPp3c3Pr/C36lwW2RH1n3A5vc/Y6o9hx3Lw42/x3YEKwvARaa2R1AeyAfWBXTqhuI7OxsbrjhBgoKCnj99dfZsmULzz4b+flYUlJCWVlZgisMh4obEx09GrnyrliXiPJyqHgpffFF5Hv05ZeRH3Z1mZnRqlUrZs6cyVVXXUWbNm1CNRe7NqpyxT0ImASsN7O1QdtNwEQz60VkqORjYCaAu280s0XAu0ApcKW7K2FqKCkpidGjRzN69Gi++OIL9u7dC8DixYt56623cHeefvppDhw4UDGrp8Fy/1fYrF4NO3ZEQnvhwkgYlZTAvn2JrTHRKj5pYf/+yNU1wNq18I9/RNb37Yt8n+o6M2P8+PHccccdtGvXjqSkhvWWlEqD293/DpxooOi5Uzzmt8Bva1GXnEDjxo2P/Rp49dVXA+Du3HTTTRw9epQHH3yQzZs3U1JSwvLlyxvUpxFu3QqPPw4V923eufNfV9cS+QH2+uvwv/8LBw5EhkC2f20AMxyGDBnC9ddfz5AhQ2jWrFmiy0kIfVZJyJkZZ511FgC33347AKWlpaxatYqSkhI+/PBD7rvvPgD++c9/UlxcfNLnCrOlS+EPf0h0FXXXF1/AnDnh/Y0jNTWVfv36cdNNNzF06FAyMjISXVJCKbjroZSUFM477zwgcnVy+eWXA7Bhwwbef/99AObOncv27dvZsWMHBw8eTFSpIpXq2bMnN998M6NGjSI5ObnezhSpDgV3PRf9Ii8oKKCgoAB3Z8yYMQC8/PLLbNmyhbKyMubOncu+ffsoLS3ls88+S1TJIgC0bNmS6dOnM336dM4880wFdhQFdwNkZsfu4jFixAhGjBiBuzN16lTcnZ07dzJ//nwA1qxZwyuvvEJZWRlHNRVDToOmTZsybdo0rrnmGvLy8urEHWfqGn1HBIiEeWZmJhD5j3PzzTcDcOjQIfbv388HH3zAggULAHjzzTfZsGFDg5/FIrHVvHlzJk2axHXXXUdeXl6DmdpXEwpuOaXMzEwyMzPJzc1lyJAhAOzYsYOdO3cyZ84clixZojcHSa0NHDiQG264gTFjxmhIpAoa1uRHiYl27dpxzjnn8NBDD7F8+XIuu+wyWrduneiyJGSSkpIoLCxk0aJFPP/884wdO1ahXUUKbqkxM6NPnz489NBDPP/880ydOpWsrKxElyV1XGpqKr179+bRRx/ltddeY9y4cTRv3jzRZYWKhkqk1syMfv36UVhYyJVXXsnKlSu599572bRpU4N6E5CcWqtWrZg+fTo9e/bkBz/4ASkpKbrCriEFt8RMUlISffr0oXfv3kyePJmFCxdyzz33sH79ekorPjBEGpTU1FQGDRrE4MGDmTp1qmaJxIi+gxJzZkZGRgbTp09n3LhxLF68mHnz5vHmm29qJkoD0aZNGy666CJGjx7NBRdc0GDfmh4vCm6Jq5YtWzJt2jTGjBnDCy+8wNy5c1m5cmXlD5TQSUpKYtiwYfTq1Ysf//jHdOrUSUMhcaLgltMiKyuLyy67jNGjR/PCCy9w991389FHH7Fjx45Elya1dNZZZ5GXl8dNN93EoEGDSEtLS3RJ9Z6CW06rli1bMnHiRCZMmMD69et54IEHePzxxxXgIdOsWTPatWvHjBkzmDBhAu3bt9fV9Wmk6YCSEGZGz549ueOOO1i+fDnXXXcdrVq1SnRZcgrJycm0adOGiy++mMWLF7Nx40Z++tOfkpubq9A+zRTcklBJSUl069aNW2+9lZUrV3LNNdc0+I/srGvS09Pp0qULt912G1u2bGHhwoVccMEFms6XQApuqRNSUlLo2rUrt99+O++88w4zZ86kadOmCoYEMTNSU1OZOHEiK1asYOXKlVx77bU0a9ZMY9h1gIJb6pSUlBTOPPNMfv/737NmzRqefPJJvvnNbya6rAalT58+3HfffbzzzjssWLCA3r17k5WVpR+idUhVbhacDrwKpAXHP+HuvzKzzsBjQGtgDTDJ3Y+aWRrwEFAIfApc4u4fx6l+qaeSkpLIz88nPz+fCy+8kGXLlnHrrbeyevVq3SQ5DrKysvjud7/LtGnT+MY3vkF2dnaiS5JTqMoV9xFgmLufA/QCRprZAOC/gTvdvSvwGTAtOH4a8FnQfmdwnEiNNW3alLFjx/LKK6/wpz/9iX79+tGoUaNElxV6qamp9OrVi4ULF/LCCy/wwAMPMHjwYIV2CFQa3B5xKNhMDRYHhgFPBO0LgLHB+phgm2D/BabfsSQG0tLSGDt2LCtWrODhhx+mf//+Gm+tgdatW3PDDTcwf/58Vq1axYQJEygsLNRQSIhUaR63mSUTGQ7pCvwP8AGwz90rPoBiG5AbrOcCWwHcvdTM9hMZTtkTw7qlgaq4e8/48eO56KKLeOqpp9iyZQurV6+mSZMmiS6vzlqzZg39+/dnwIABTJkyhY4dO+pGBSFWpeB29zKgl5m1AJ4CutX2xGY2A5gB0LFjx9o+nTRAjRs35tJLL6WsrIw9e/boc1AqkZGRQdOmTRNdhsRAtd456e77zGw5MBBoYWYpwVV3B2B7cNh2IA/YZmYpQHMif6Q8/rnmAfMA+vbtq/9xUmPJycm0bds20WWInDaVjnGbWXZwpY2ZNQaGA5uA5cC44LApwDPB+pJgm2D/y65LIRGRmKnKFXcOsCAY504CFrn7s2b2LvCYmd0MvA3cHxx/P/CwmRUBe4EJcahbRKTBqjS43X0d0PsE7R8C556g/UvgBzGpTkREvkbvnBQRCRkFt4hIyCi4RURCRsEtIhIyCm4RkZBRcIuIhIyCW0QkZBTcIiIho+AWEQkZBbeISMgouEVEQkbBLSISMgpuEZGQUXCLiISMgltEJGQU3CIiIaPgFhEJGQW3iEjIVOVmwelmtsrM3jGzjWb266B9vpl9ZGZrg6VX0G5mdpeZFZnZOjPrE+9OiIg0JFW5WfARYJi7HzKzVODvZvZ8sO96d3/iuONHAfnB0h+4J/gqIiIxUOkVt0ccCjZTg8VP8ZAxwEPB494AWphZTu1LFRERqOIYt5klm9laYBew1N1XBrt+GwyH3GlmaUFbLrA16uHbgjYREYmBKgW3u5e5ey+gA3CumfUAZgHdgH5AK+Dn1Tmxmc0ws9Vmtnr37t3VLFtEpOGq1qwSd98HLAdGuntxMBxyBHgQODc4bDuQF/WwDkHb8c81z937unvf7OzsmlUvItIAVWVWSbaZtQjWGwPDgfcqxq3NzICxwIbgIUuAycHskgHAfncvjkv1IiINUFVmleQAC8wsmUjQL3L3Z83sZTPLBgxYC/xncPxzwHeAIuAw8MPYly0i0nBVGtzuvg7ofYL2YSc53oEra1+aiIiciN45KSISMgpuEZGQUXCLiISMgltEJGQU3CIiIaPgFhEJGQW3iEjIKLhFREJGwS0iEjIKbhGRkFFwi4iEjIJbRCRkFNwiIiGj4BYRCRkFt4hIyCi4RURCRsEtIhIyCm4RkZBRcIuIhIyCW0QkZBTcIiIho+AWEQkZc/dE14CZHQQ2J7qOOMkC9iS6iDior/2C+ts39StcznD37BPtSDndlZzEZnfvm+gi4sHMVtfHvtXXfkH97Zv6VX9oqEREJGQU3CIiIVNXgnteoguIo/rat/raL6i/fVO/6ok68cdJERGpurpyxS0iIlWU8OA2s5FmttnMiszsxkTXU11m9oCZ7TKzDVFtrcxsqZltCb62DNrNzO4K+rrOzPokrvJTM7M8M1tuZu+a2UYz+0nQHuq+mVm6ma0ys3eCfv06aO9sZiuD+h83s0ZBe1qwXRTs75TI+itjZslm9raZPRts15d+fWxm681srZmtDtpC/VqsjYQGt5klA/8DjAK6AxPNrHsia6qB+cDI49puBJa5ez6wLNiGSD/zg2UGcM9pqrEmSoHr3L07MAC4Mvi3CXvfjgDD3P0coBcw0swGAP8N3OnuXYHPgGnB8dOAz4L2O4Pj6rKfAJuitutLvwC+5e69oqb+hf21WHPunrAFGAi8GLU9C5iVyJpq2I9OwIao7c1ATrCeQ2SeOsB9wMQTHVfXF+AZYHh96hvQBHgL6E/kDRwpQfux1yXwIjAwWE8JjrNE136S/nQgEmDDgGcBqw/9Cmr8GMg6rq3evBaruyR6qCQX2Bq1vS1oC7u27l4crO8A2gbroexv8Gt0b2Al9aBvwXDCWmAXsBT4ANjn7qXBIdG1H+tXsH8/0Pr0Vlxl/xe4ASgPtltTP/oF4MBLZrbGzGYEbaF/LdZUXXnnZL3l7m5moZ26Y2aZwJPAte5+wMyO7Qtr39y9DOhlZi2Ap4BuCS6p1szsu8Aud19jZkMTXU8cDHb37WbWBlhqZu9F7wzra7GmEn3FvR3Ii9ruELSF3U4zywEIvu4K2kPVXzNLJRLaj7j74qC5XvQNwN33AcuJDCG0MLOKC5no2o/1K9jfHPj0NJdaFYOA75nZx8BjRIZL/h/h7xcA7r49+LqLyA/bc6lHr8XqSnRwvwnkB3/5bgRMAJYkuKZYWAJMCdanEBkfrmifHPzVewCwP+pXvTrFIpfW9wOb3P2OqF2h7puZZQdX2phZYyLj9puIBPi44LDj+1XR33HAyx4MnNYl7j7L3Tu4eyci/49edvf/IOT9AjCzDDNrWrEOjAA2EPLXYq0kepAd+A7wPpFxxl8kup4a1P8oUAyUEBlLm0ZkrHAZsAX4K9AqONaIzKL5AFgP9E10/afo12Ai44rrgLXB8p2w9w3oCbwd9GsD8H+C9i7AKqAI+BOQFrSnB9tFwf4uie5DFfo4FHi2vvQr6MM7wbKxIifC/lqszaJ3ToqIhEyih0pERKSaFNwiIiGj4BYRCRkFt4hIyCi4RURCRsEtIhIyCm4RkZBRcIuIhMz/B796LGPnXJnYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4oDiYq_ZzMk"
      },
      "source": [
        "## Defining RL Agent Things\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-FnnadCX5eT"
      },
      "source": [
        "### Defining the Rollout Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AId5jCKZaPW-"
      },
      "source": [
        "\n",
        "\n",
        "# class RolloutStorage():\n",
        "#     def __init__(self, rollout_size, obs_size):\n",
        "#         '''\n",
        "#         rollout_size: The size of the rollout buffer\n",
        "#         obs_size: The dimension of the observation vector\n",
        "#         '''\n",
        "#         self.rollout_size = rollout_size\n",
        "#         self.obs_size = obs_size\n",
        "#         self.reset()\n",
        "        \n",
        "#     def insert(self, step, done, action, log_prob, reward, obs):\n",
        "#         '''\n",
        "#         Inserting the transition at the current step number in the environment.\n",
        "#         '''\n",
        "#         self.done[step].copy_(done)\n",
        "#         self.actions[step].copy_(action)\n",
        "#         self.log_probs[step].copy_(log_prob)\n",
        "#         self.rewards[step].copy_(reward)\n",
        "#         self.obs[step].copy_(obs)\n",
        "        \n",
        "#     def reset(self):\n",
        "#         '''\n",
        "#         Initialize all storage buffers with zeros.\n",
        "#         '''\n",
        "#         self.done = torch.zeros(self.rollout_size, 1)\n",
        "#         self.returns = torch.zeros(self.rollout_size+1, 1, requires_grad=False)\n",
        "#         self.actions = torch.zeros(self.rollout_size, 1, dtype=torch.int64)  # Assuming Discrete Action Space\n",
        "#         self.log_probs = torch.zeros(self.rollout_size, 1)\n",
        "#         self.rewards = torch.zeros(self.rollout_size, 1)\n",
        "#         self.obs = torch.zeros(self.rollout_size, self.obs_size)\n",
        "        \n",
        "#     def compute_returns(self, gamma):\n",
        "#         '''\n",
        "#         Compute cumulative discounted returns from the current state to the end of the episode.\n",
        "#         '''\n",
        "#         self.last_done = (self.done == 1).nonzero().max()  # Find point of last episode's end in buffer.\n",
        "#         self.returns[self.last_done+1] = 0.  # Initialize the return at the end of last episode to be 0.\n",
        "\n",
        "#         # Accumulate discounted returns using dynamic programming.\n",
        "#         # Cumulative return = reward from current step + discounted future reward until end of episode.\n",
        "#         for step in reversed(range(self.last_done+1)):\n",
        "#             self.returns[step] = self.rewards[step] + \\\n",
        "#                                 self.returns[step + 1] * gamma * (1 - self.done[step])\n",
        "        \n",
        "#     def batch_sampler(self, batch_size, get_old_log_probs=False):\n",
        "#         '''\n",
        "#         Create a batch sampler of indices. Return actions, returns, observation for training.\n",
        "#         get_old_log_probs: This is required for PPO to recall the log_prob of the action w.r.t.\n",
        "#                            the policy that generated this transition.\n",
        "#         '''\n",
        "#         sampler = BatchSampler(\n",
        "#             SubsetRandomSampler(range(self.last_done)),\n",
        "#             batch_size,\n",
        "#             drop_last=True)\n",
        "#         for indices in sampler:\n",
        "#             if get_old_log_probs:\n",
        "#                 yield self.actions[indices], self.returns[indices], self.obs[indices], self.log_probs[indices]\n",
        "#             else:\n",
        "#                 yield self.actions[indices], self.returns[indices], self.obs[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlhSh-FwYGSL"
      },
      "source": [
        "### Policy Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqINymrpYCiS"
      },
      "source": [
        "\n",
        "# # from utils import count_model_params\n",
        "\n",
        "# class ActorNetwork(nn.Module):\n",
        "#     def __init__(self, num_inputs, num_actions, hidden_dim):\n",
        "#         super().__init__()\n",
        "#         self.num_actions = num_actions\n",
        "        \n",
        "#         ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#         ### 1. Build the Actor network as a torch.nn.Sequential module                   ###\n",
        "#         ###    with the following layers:                                                ###\n",
        "#         ###        (1) a Linear layer mapping from input dimension to hidden dimension   ###\n",
        "#         ###        (2) a Tanh non-linearity                                              ###\n",
        "#         ###        (3) a Linear layer mapping from hidden dimension to hidden dimension  ###\n",
        "#         ###        (4) a Tanh non-linearity                                              ###\n",
        "#         ###        (5) a Linear layer mapping from hidden dimension to number of actions ###\n",
        "#         ### HINT: We do not need an activation on the output, because the actor is       ###\n",
        "#         ###       predicting logits for categorical distribution.                        ###\n",
        "#         ####################################################################################\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(num_inputs, hidden_dim),\n",
        "#             nn.Tanh(),\n",
        "#             nn.Linear(hidden_dim, hidden_dim),\n",
        "#             nn.Tanh(),\n",
        "#             nn.Linear(hidden_dim, num_actions)\n",
        "#         )\n",
        "#         ################################# END OF YOUR CODE #################################\n",
        "        \n",
        "#     def forward(self, state):\n",
        "#         x = self.fc(state)\n",
        "#         return x\n",
        "\n",
        "# class Policy():\n",
        "#     '''\n",
        "#     Policy Class used for acting in the environment and updating the policy network.\n",
        "#     '''\n",
        "#     def __init__(self, num_inputs, num_actions, hidden_dim, learning_rate,\n",
        "#                  batch_size, policy_epochs, entropy_coef=0.001):\n",
        "#         self.actor = ActorNetwork(num_inputs, num_actions, hidden_dim)\n",
        "#         self.optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
        "#         self.batch_size = batch_size\n",
        "#         self.policy_epochs = policy_epochs\n",
        "#         self.entropy_coef = entropy_coef\n",
        "\n",
        "#     def act(self, state):\n",
        "#         ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#         ### 1. Run the actor network on the current state to get the action logits       ###\n",
        "#         ### 2. Build a Categorical(...) instance from the logits                         ###\n",
        "#         ### 3. Sample an action using the built-in sample() function of distribution.    ###\n",
        "#         ### Documentation of Categorical:                                                ###\n",
        "#         ### https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical\n",
        "#         ####################################################################################\n",
        "#         logits = self.actor(state)\n",
        "#         dist =  Categorical(logits=logits)\n",
        "#         action = dist.sample()\n",
        "#         ################################# END OF YOUR CODE #################################\n",
        "#         log_prob = dist.log_prob(action)\n",
        "#         return action, log_prob\n",
        "    \n",
        "#     def evaluate_actions(self, state, action):\n",
        "#         '''\n",
        "#         Evaluate the log probability of an action under the policy's output\n",
        "#         distribution for a given state.\n",
        "        \n",
        "#         state -> tensor: [batch_size, obs_size]\n",
        "#         action -> tensor: [batch_size, 1]\n",
        "#         '''\n",
        "#         ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#         ### This function is used for policy update to evaluate log_prob and entropy of  ###\n",
        "#         ### actor network.                                                               ###\n",
        "#         ### TODO: \n",
        "#         ### 1. Compute logits and distribution for the given state (just like above).\n",
        "#         ### 2. Compute log probability of the given action under this distribution.\n",
        "#         ###    Hint: Input to the distribution should be in the shape [batch_size].\n",
        "#         ###          You may find `action.squeeze(...)` helpful.\n",
        "#         ### 3. Compute the entropy of the distribution.\n",
        "#         ####################################################################################\n",
        "#         logits = self.actor(state)\n",
        "#         dist =  Categorical(logits=logits)\n",
        "#         log_prob = dist.log_prob(action.squeeze())\n",
        "#         entropy = dist.entropy()\n",
        "#         ################################# END OF YOUR CODE #################################\n",
        "#         return log_prob.view(-1, 1), entropy.view(-1, 1)\n",
        "    \n",
        "#     def update(self, rollouts):\n",
        "#         '''\n",
        "#         Performing policy gradient update with maximum entropy regularization\n",
        "        \n",
        "#         rollouts -> The storage buffer\n",
        "#         self.policy_epochs -> Number of times we train over the storage buffer\n",
        "#         '''\n",
        "#         for epoch in range(self.policy_epochs):\n",
        "#             data = rollouts.batch_sampler(self.batch_size)\n",
        "            \n",
        "#             for sample in data:\n",
        "#                 actions_batch, returns_batch, obs_batch = sample\n",
        "                \n",
        "#                 # Compute Log probabilities and entropy for each sampled (state, action)\n",
        "#                 log_probs_batch, entropy_batch = self.evaluate_actions(obs_batch, actions_batch)\n",
        "    \n",
        "#                 ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#                 ### 4. Compute the mean loss for the policy update using action log-             ###\n",
        "#                 ###     probabilities and policy returns                                         ###\n",
        "#                 ### 5. Compute the mean entropy for the policy update                            ###\n",
        "#                 ###    *HINT*: PyTorch optimizer is used to minimize by default.                 ###\n",
        "#                 ###     The trick to maximize a quantity is to negate its corresponding loss.    ###\n",
        "#                 ####################################################################################\n",
        "#                 policy_loss = -torch.mean(log_probs_batch * returns_batch)\n",
        "#                 entropy_loss = -torch.mean(entropy_batch)\n",
        "#                 ################################# END OF YOUR CODE #################################\n",
        "                \n",
        "#                 loss = policy_loss + self.entropy_coef * entropy_loss\n",
        "                \n",
        "#                 self.optimizer.zero_grad()\n",
        "#                 loss.backward(retain_graph=False)\n",
        "#                 self.optimizer.step()\n",
        "                \n",
        "#     @property\n",
        "#     def num_params(self):\n",
        "#       return sum(p.numel() for p in self.actor.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjPkYG2eefs5"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_rVgxT5Ym4J"
      },
      "source": [
        "# def instantiate(params_in, nonwrapped_env=None, seed=123):\n",
        "#     '''\n",
        "#     SETTING SEED: it is good practice to set seeds when running experiments to keep results comparable\n",
        "#     '''\n",
        "#     np.random.seed(seed)\n",
        "#     torch.manual_seed(seed)\n",
        "    \n",
        "#     params = copy.deepcopy(params_in)\n",
        "\n",
        "#     '''\n",
        "#     1. Instantiate the environment\n",
        "#     # If an environment is given as input to this function, we directly use that.\n",
        "#     # Else, we use the environment specified in `params`.\n",
        "#     '''\n",
        "#     if nonwrapped_env is None:\n",
        "#         nonwrapped_env = gym.make(params.env_name)\n",
        "#     env = nonwrapped_env\n",
        "    \n",
        "#     obs_size = env.observation_space.shape[0]\n",
        "#     num_actions = env.action_space.n\n",
        "#     env.seed(seed)   # Required for reproducibility in stochastic environments.\n",
        "\n",
        "#     '''\n",
        "#     2. Instantiate Rollout Buffer and Policy\n",
        "#     '''\n",
        "#     rollouts = RolloutStorage(params.rollout_size, obs_size)\n",
        "#     policy_class = params.policy_params.pop('policy_class')    \n",
        "#     policy = policy_class(obs_size, num_actions, **params.policy_params)\n",
        "    \n",
        "#     return env, rollouts, policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6DO6eW5Z5YA"
      },
      "source": [
        "# def train(env, rollouts, policy, params):\n",
        "#     rollout_time, update_time = AverageMeter(), AverageMeter()  # Loggers\n",
        "#     rewards, success_rate = [], []\n",
        "\n",
        "#     print(\"Training model with {} parameters...\".format(policy.num_params))\n",
        "\n",
        "#     '''\n",
        "#     Training Loop\n",
        "#     '''\n",
        "#     for j in range(params.num_updates):\n",
        "#         ## Initialization\n",
        "#         avg_eps_reward, avg_success_rate = AverageMeter(), AverageMeter()\n",
        "#         done = False\n",
        "#         prev_obs = env.reset()\n",
        "#         prev_obs = torch.tensor(prev_obs, dtype=torch.float32)\n",
        "#         eps_reward = 0.\n",
        "#         start_time = time.time()\n",
        "        \n",
        "#         ## Collect rollouts\n",
        "#         for step in range(rollouts.rollout_size):\n",
        "#             if done:\n",
        "#                 # Store episode statistics\n",
        "#                 avg_eps_reward.update(eps_reward)\n",
        "#                 if 'success' in info: \n",
        "#                     avg_success_rate.update(int(info['success']))\n",
        "\n",
        "#                 # Reset Environment\n",
        "#                 obs = env.reset()\n",
        "#                 obs = torch.tensor(obs, dtype=torch.float32)\n",
        "#                 eps_reward = 0.\n",
        "#             else:\n",
        "#                 obs = prev_obs\n",
        "\n",
        "#             ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#             ### 1. Call the policy to get the action for the current observation,            ###\n",
        "#             ### 2. Take one step in the environment (using the policy's action)              ###\n",
        "#             ####################################################################################\n",
        "#             action, log_prob = policy.act(obs)\n",
        "#             # action = \n",
        "#             obs, reward, done, info = env.step(action.item())\n",
        "#             ################################# END OF YOUR CODE #################################\n",
        "\n",
        "            \n",
        "#             ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#             ### 3. Insert the sample <done, action, log_prob, reward, prev_obs> in the       ###\n",
        "#             ###    rollout storage. (requires just 1 line)                                   ###\n",
        "#             ### HINT:                                                                        ###\n",
        "#             ### - 'done' and 'reward' need to be converted to float32 tensors first          ###\n",
        "#             ### - Remember we are storing the previous observation because                   ###\n",
        "#             ###   that's what decided the policy's action                                    ###\n",
        "#             ####################################################################################\n",
        "#             rollouts.insert(step, torch.tensor(done, dtype=torch.float32), action, log_prob, \n",
        "#                             torch.tensor(reward, dtype=torch.float32), prev_obs)\n",
        "#             ################################# END OF YOUR CODE #################################\n",
        "            \n",
        "#             prev_obs = torch.tensor(obs, dtype=torch.float32)\n",
        "#             eps_reward += reward\n",
        "        \n",
        "#         ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#         ### 4. Use the rollout buffer's function to compute the returns for all          ###\n",
        "#         ###    stored rollout steps. Discount factor is given in 'params'                ###\n",
        "#         ### HINT: This requires just 1 line of code.                                     ###\n",
        "#         ####################################################################################\n",
        "#         rollouts.compute_returns(params.discount)\n",
        "#         ################################# END OF YOUR CODE #################################\n",
        "        \n",
        "#         rollout_done_time = time.time()\n",
        "\n",
        "#         ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#         ### 5. Call the policy's update function using the collected rollouts            ###\n",
        "#         ####################################################################################\n",
        "#         policy.update(rollouts)\n",
        "#         ################################# END OF YOUR CODE #################################\n",
        "\n",
        "#         update_done_time = time.time()\n",
        "#         rollouts.reset()\n",
        "\n",
        "#         ## log metrics\n",
        "#         rewards.append(avg_eps_reward.avg)\n",
        "#         if avg_success_rate.count > 0:\n",
        "#             success_rate.append(avg_success_rate.avg)\n",
        "#         rollout_time.update(rollout_done_time - start_time)\n",
        "#         update_time.update(update_done_time - rollout_done_time)\n",
        "#         print('it {}: avgR: {:.3f} -- rollout_time: {:.3f}sec -- update_time: {:.3f}sec'.format(j, avg_eps_reward.avg, \n",
        "#                                                                                                 rollout_time.avg, \n",
        "#                                                                                                 update_time.avg))\n",
        "#         if j % params.plotting_iters == 0 and j != 0:\n",
        "#             plot_learning_curve(rewards, success_rate, params.num_updates)\n",
        "#             # log_policy_rollout(policy, params.env_name, pytorch_policy=True)\n",
        "#     clear_output()   # this removes all training outputs to keep the notebook clean, DON'T REMOVE THIS LINE!\n",
        "#     return rewards, success_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs4swPjOZP2u"
      },
      "source": [
        "# class ParamDict(dict):\n",
        "#     __setattr__ = dict.__setitem__\n",
        "#     def __getattr__(self, attr):\n",
        "#         # Take care that getattr() raises AttributeError, not KeyError.\n",
        "#         # Required e.g. for hasattr(), deepcopy and OrderedDict.\n",
        "#         try:\n",
        "#             return self.__getitem__(attr)\n",
        "#         except KeyError:\n",
        "#             raise AttributeError(\"Attribute %r not found\" % attr)\n",
        "#     def __getstate__(self): return self\n",
        "#     def __setstate__(self, d): self = d\n",
        "\n",
        "# def plot_grid_std_learning_curves(d, num_it):\n",
        "#     for i, key in enumerate(d):\n",
        "#         ax = plt.subplot(2, 2, i+1)\n",
        "#         rewards, success_rates = d[key]\n",
        "#         plot_std_learning_curves(rewards, success_rates, num_it, no_show=True)\n",
        "#         ax.set_title(key)\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# def plot_std_learning_curves(rewards, success_rates, num_it, no_show=False):\n",
        "#     r, sr = np.asarray(rewards), np.asarray(success_rates)\n",
        "#     df = pd.DataFrame(r).melt()\n",
        "#     sns.lineplot(x=\"variable\", y=\"value\", data=df, label='reward/eps')\n",
        "#     df = pd.DataFrame(sr).melt()\n",
        "#     sns.lineplot(x=\"variable\", y=\"value\", data=df, label='success rate')\n",
        "#     plt.xlabel(\"Training iterations\")\n",
        "#     plt.ylabel(\"\")\n",
        "#     plt.xlim([0, num_it])\n",
        "#     plt.ylim([-200, 100])\n",
        "#     plt.legend()\n",
        "#     plt.grid('on')\n",
        "#     if not no_show:\n",
        "#         plt.show()\n",
        "\n",
        "\n",
        "# def plot_learning_curve(rewards, success_rate, num_it, plot_std=False):\n",
        "#     if plot_std:\n",
        "#         # plots shaded regions if list of reward timeseries is given\n",
        "#         plot_std_learning_curves(rewards, success_rate, num_it)\n",
        "#     else:\n",
        "#         plt.plot(rewards, label='reward/eps')\n",
        "#         if success_rate:\n",
        "#             plt.plot(success_rate, label='success rate')\n",
        "#             plt.legend()\n",
        "#         else:\n",
        "#             plt.ylabel('return / eps')\n",
        "#         plt.ylim([-200, 100])\n",
        "#         plt.xlim([0, num_it - 1])\n",
        "#         plt.xlabel('train iter')\n",
        "#         plt.grid('on')\n",
        "#         plt.show()\n",
        "\n",
        "# class AverageMeter(object):\n",
        "#     \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.reset()\n",
        "\n",
        "#     def reset(self):\n",
        "#         self.val = 0\n",
        "#         self.avg = 0\n",
        "#         self.sum = 0\n",
        "#         self.count = 0\n",
        "\n",
        "#     def update(self, val, n=1):\n",
        "#         self.val = val\n",
        "#         self.sum += val * n\n",
        "#         self.count += n\n",
        "#         self.avg = self.sum / self.count\n",
        "\n",
        "# def gen_wrapped_env(env_name):\n",
        "#     return wrap_env(gym.make(env_name))\n",
        "\n",
        "\n",
        "# # This function plots videos of rollouts (episodes) of a given policy and environment\n",
        "# def log_policy_rollout(policy, env_name, pytorch_policy=False, level=None):\n",
        "#     # Create environment with flat observation\n",
        "#     env = gen_wrapped_env(env_name)\n",
        "\n",
        "#     if level != None:\n",
        "#       env.load_terrain(level)\n",
        "#     # Initialize environment\n",
        "#     observation = env.reset()\n",
        "\n",
        "#     done = False\n",
        "#     episode_reward = 0\n",
        "#     episode_length = 0\n",
        "\n",
        "#     # Run until done == True\n",
        "#     while not done:\n",
        "#       # Take a step\n",
        "#         if pytorch_policy: \n",
        "#             observation = torch.tensor(observation, dtype=torch.float32)\n",
        "#             action = policy.act(observation)[0].data.cpu().numpy()\n",
        "#         else:\n",
        "#             action = policy.act(observation)[0]\n",
        "#         observation, reward, done, info = env.step(action)\n",
        "\n",
        "#         episode_reward += reward\n",
        "#         episode_length += 1\n",
        "\n",
        "#     img = env.render('rgb_array')\n",
        "#     plt.figure()\n",
        "#     plt.imshow(img)\n",
        "#     plt.title('REWARD {}, LENGTH {}'.format(episode_reward, episode_length))\n",
        "#     # print('Total reward:', episode_reward)\n",
        "#     # print('Total length:', episode_length)\n",
        "\n",
        "#     # env.close()\n",
        "    \n",
        "#     # show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5YWVajSejzH"
      },
      "source": [
        "### Vanilla Policy Gradients \n",
        "(Just Actor Network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzB8SUqZZRU7"
      },
      "source": [
        "# # hyperparameters\n",
        "# policy_params = ParamDict(\n",
        "#     policy_class = Policy,    # Policy class to use (replaced later)     \n",
        "#     hidden_dim = 32,          # dimension of the hidden state in actor network\n",
        "#     learning_rate = 1e-3,     # learning rate of policy update\n",
        "#     batch_size = 1024,        # batch size for policy update\n",
        "#     policy_epochs = 4,        # number of epochs per policy update\n",
        "#     entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
        "# )\n",
        "# params = ParamDict(\n",
        "#     policy_params = policy_params,\n",
        "#     rollout_size = 2050,      # number of collected rollout steps per policy update\n",
        "#     num_updates = 200,         # number of training policy iterations\n",
        "#     discount = 0.99,          # discount factor\n",
        "#     plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
        "#     env_name = 'LunarLander-v2',  # we are using a tiny environment here for testing\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsZLgJ2LZdzK"
      },
      "source": [
        "# import copy\n",
        "# env, rollouts, policy = instantiate(params)\n",
        "# env.load_terrain([2.5668, 0.8473, 3.2891, 1.4700, 2.5869, 2.4287, 2.6033, 2.1752, 4.6830, 0.3597, 3.1192])\n",
        "# rewards, success_rate = train(env, rollouts, policy, params)\n",
        "# print(\"Training completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-31M-gggMAE"
      },
      "source": [
        "# # final reward + policy plotting for easier evaluation\n",
        "# plot_learning_curve(rewards, success_rate, params.num_updates)\n",
        "# for _ in range(3):\n",
        "#     log_policy_rollout(policy, params.env_name, pytorch_policy=True, level=[2.5668, 0.8473, 3.2891, 1.4700, 2.5869, 2.4287, 2.6033, 2.1752, 4.6830, 0.3597, 3.1192])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twp72n72ios4"
      },
      "source": [
        "### Actor-Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncsn3s2Ri0TY"
      },
      "source": [
        "# class CriticNetwork(nn.Module):\n",
        "#     def __init__(self, num_inputs, hidden_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#         ### 1. Build the Actor network as a torch.nn.Sequential module                   ###\n",
        "#         ###    with the following layers:                                                ###\n",
        "#         ###        (1) a Linear layer mapping from input dimension to hidden dimension   ###\n",
        "#         ###        (2) a Tanh non-linearity                                              ###\n",
        "#         ###        (3) a Linear layer mapping from hidden dimension to hidden dimension  ###\n",
        "#         ###        (4) a Tanh non-linearity                                              ###\n",
        "#         ###        (5) a Linear layer mapping from hidden dimension to 1                 ###\n",
        "#         ### HINT: We do not need an activation on the output, because the actor is       ###\n",
        "#         ###       predicting a value, which can be any real number                       ###\n",
        "#         ####################################################################################\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(num_inputs,hidden_dim),\n",
        "#             nn.Tanh(),\n",
        "#             nn.Linear(hidden_dim, hidden_dim),\n",
        "#             nn.Tanh(),\n",
        "#             nn.Linear(hidden_dim,1)\n",
        "#         )\n",
        "#         ################################# END OF YOUR CODE #################################\n",
        "\n",
        "#     def forward(self, state):\n",
        "#         x = self.fc(state)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class ACPolicy(Policy):\n",
        "#     def __init__(self, num_inputs, num_actions, hidden_dim, learning_rate, batch_size, policy_epochs,\n",
        "#                  entropy_coef=0.001, critic_coef=0.5):\n",
        "#         super().__init__(num_inputs, num_actions, hidden_dim, learning_rate, batch_size, policy_epochs, entropy_coef)\n",
        "\n",
        "#         self.critic = CriticNetwork(num_inputs, hidden_dim)\n",
        "        \n",
        "#         ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#         ### Create a common optimizer for actor and critic with the given learning rate  ###\n",
        "#         ### (requires 1-line of code)                                                    ###\n",
        "#         ####################################################################################\n",
        "#         self.optimizer = optim.Adam(list(self.critic.parameters()) + list(self.actor.parameters()), lr=learning_rate)\n",
        "#         ################################# END OF YOUR CODE #################################\n",
        "\n",
        "#         self.critic_coef = critic_coef\n",
        "        \n",
        "#     def update(self, rollouts): \n",
        "#         for epoch in range(self.policy_epochs):\n",
        "#             data = rollouts.batch_sampler(self.batch_size)\n",
        "            \n",
        "#             for sample in data:\n",
        "#                 actions_batch, returns_batch, obs_batch = sample\n",
        "#                 log_probs_batch, entropy_batch = self.evaluate_actions(obs_batch, actions_batch)\n",
        "\n",
        "#                 value_batch = self.critic(obs_batch)\n",
        "#                 advantage = returns_batch - value_batch.detach()\n",
        "\n",
        "                \n",
        "#                 ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#                 ### 1. Compute the mean loss for the policy update using action log-             ###\n",
        "#                 ###     probabilities and advantages.                                            ###\n",
        "#                 ### 2. Compute the mean entropy for the policy update                            ###\n",
        "#                 ### 3. Compute the critic loss as MSE loss between estimated value and expected  ###\n",
        "#                 ###     returns.                                                                 ###\n",
        "#                 ###    *HINT*: Carefully select the signs of each of the losses .                ###\n",
        "#                 ####################################################################################\n",
        "#                 policy_loss = -torch.mean(log_probs_batch*advantage)\n",
        "#                 entropy_loss = -torch.mean(entropy_batch)\n",
        "#                 critic_loss = nn.MSELoss()(value_batch, returns_batch)\n",
        "#                 ################################# END OF YOUR CODE #################################\n",
        "                \n",
        "#                 loss = policy_loss + \\\n",
        "#                         self.critic_coef * critic_loss + \\\n",
        "#                         self.entropy_coef * entropy_loss\n",
        "\n",
        "#                 self.optimizer.zero_grad()\n",
        "#                 loss.backward()\n",
        "#                 self.optimizer.step()\n",
        "                \n",
        "#     @property\n",
        "#     def num_params(self):\n",
        "#         return super().num_params + sum(p.numel() for p in self.critic.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooK_0MIBkkKP"
      },
      "source": [
        "# n_seeds=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkPSw0TDkgL2"
      },
      "source": [
        "# # hyperparameters\n",
        "# policy_params = ParamDict(\n",
        "#     policy_class = ACPolicy,  # Policy class to use (replaced later)\n",
        "#     hidden_dim = 32,          # dimension of the hidden state in actor network\n",
        "#     learning_rate = 1e-3,     # learning rate of policy update\n",
        "#     batch_size = 1024,        # batch size for policy update\n",
        "#     policy_epochs = 4,        # number of epochs per policy update\n",
        "#     entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
        "#     critic_coef = 0.5         # Coefficient of critic loss when weighted against actor loss\n",
        "# )\n",
        "# params = ParamDict(\n",
        "#     policy_params = policy_params,\n",
        "#     rollout_size = 2050,      # number of collected rollout steps per policy update\n",
        "#     num_updates = 150,        # number of training policy iterations\n",
        "#     discount = 0.99,          # discount factor\n",
        "#     plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
        "#     env_name = 'LunarLander-v2',\n",
        "# )\n",
        "\n",
        "# rewards_ac, success_rates_ac = [], []\n",
        "# for i in range(n_seeds):\n",
        "#     print(\"Start training run {}!\".format(i))\n",
        "#     env, rollouts, policy = instantiate(params, nonwrapped_env=gym.make(\"LunarLander-v2\"), seed=i)\n",
        "#     r, sr = train(env, rollouts, policy, params)\n",
        "#     rewards_ac.append(r); success_rates_ac.append(sr)\n",
        "# print('All training runs completed!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-W8jTxuk9eK"
      },
      "source": [
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "\n",
        "# plot_learning_curve(rewards_ac, success_rates_ac, params.num_updates, plot_std=True)\n",
        "\n",
        "# for _ in range(3):\n",
        "#     log_policy_rollout(policy, 'LunarLander-v2', pytorch_policy=True, level=[2.5668, 0.8473, 3.2891, 1.4700, 2.5869, 2.4287, 2.6033, 2.1752, 4.6830, 0.3597, 3.1192])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TCGhqTgkw7b"
      },
      "source": [
        "### PPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UCAXXiekyko"
      },
      "source": [
        "# class PPO(ACPolicy):       \n",
        "#     def update(self, rollouts): \n",
        "#         self.clip_param = 0.2\n",
        "#         for epoch in range(self.policy_epochs):\n",
        "#             data = rollouts.batch_sampler(self.batch_size, get_old_log_probs=True)\n",
        "            \n",
        "#             for sample in data:\n",
        "#                 actions_batch, returns_batch, obs_batch, old_log_probs_batch = sample\n",
        "#                 log_probs_batch, entropy_batch = self.evaluate_actions(obs_batch, actions_batch)\n",
        "                \n",
        "#                 value_batch = self.critic(obs_batch)\n",
        "                \n",
        "#                 advantage = returns_batch - value_batch.detach()\n",
        "#                 old_log_probs_batch = old_log_probs_batch.detach()\n",
        "\n",
        "#                 ############################## TODO: YOUR CODE BELOW ###############################\n",
        "#                 ### Compute the following terms by following the equations given above           ###\n",
        "#                 ### Useful functions: torch.exp(...), torch.clamp(...)\n",
        "#                 ### Note: self.clip_param is the c in the above equations                        ###\n",
        "#                 ### Compute the following terms by following the equations given above           ###\n",
        "#                 ####################################################################################\n",
        "#                 ratio = torch.exp(log_probs_batch - old_log_probs_batch)\n",
        "#                 surr1 = ratio * advantage\n",
        "#                 surr2 = torch.clamp(ratio, 1-self.clip_param, 1+self.clip_param) * advantage\n",
        "\n",
        "#                 policy_loss = -torch.mean(torch.min(surr1, surr2))\n",
        "# #                 print(policy_loss)\n",
        "#                 entropy_loss = -torch.mean(entropy_batch)\n",
        "#                 critic_loss = nn.MSELoss()(value_batch, returns_batch)\n",
        "#                 ################################# END OF YOUR CODE #################################\n",
        "\n",
        "#                 loss = policy_loss + \\\n",
        "#                         self.critic_coef * critic_loss + \\\n",
        "#                         self.entropy_coef * entropy_loss\n",
        "\n",
        "#                 self.optimizer.zero_grad()\n",
        "#                 loss.backward(retain_graph=False)\n",
        "#                 self.optimizer.step()       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv3cxOYvk2we"
      },
      "source": [
        "# # hyperparameters\n",
        "# policy_params = ParamDict(\n",
        "#     policy_class = PPO,  # Policy class to use (replaced later)\n",
        "#     hidden_dim = 32,          # dimension of the hidden state in actor network\n",
        "#     learning_rate = 1e-3,     # learning rate of policy update\n",
        "#     batch_size = 1024,        # batch size for policy update\n",
        "#     policy_epochs = 4,        # number of epochs per policy update\n",
        "#     entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
        "#     critic_coef = 0.5         # Coefficient of critic loss when weighted against actor loss\n",
        "# )\n",
        "# params = ParamDict(\n",
        "#     policy_params = policy_params,\n",
        "#     rollout_size = 2050,      # number of collected rollout steps per policy update\n",
        "#     num_updates = 150,        # number of training policy iterations\n",
        "#     discount = 0.99,          # discount factor\n",
        "#     plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
        "#     env_name = 'LunarLander-v2',  # we are using a tiny environment here for testing\n",
        "# )\n",
        "\n",
        "# rewards_ppo, success_rates_ppo = [], []\n",
        "# for i in range(n_seeds):\n",
        "#     print(\"Start training run {}!\".format(i))\n",
        "#     env, rollouts, policy = instantiate(params, nonwrapped_env=gym.make(\"LunarLander-v2\"), seed=i)\n",
        "#     r, sr = train(env, rollouts, policy, params)\n",
        "#     rewards_ppo.append(r); success_rates_ppo.append(sr)\n",
        "# print('All training runs completed!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEpxYz7Dk8Vq"
      },
      "source": [
        "# plot_learning_curve(rewards_ppo, success_rates_ppo, params.num_updates, plot_std=True)\n",
        "# for _ in range(3):\n",
        "#     log_policy_rollout(policy, params.env_name, pytorch_policy=True, level=[2.5668, 0.8473, 3.2891, 1.4700, 2.5869, 2.4287, 2.6033, 2.1752, 4.6830, 0.3597, 3.1192])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmRQTyz391sz"
      },
      "source": [
        "## stable baselines Library PPO Implementation (more efficient theoretically)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ4zm4-mEKHm",
        "outputId": "eb2ff9cf-f321-40c6-fdc6-fdcb3521693e"
      },
      "source": [
        "!pip install stable-baselines3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (0.17.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.1.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.18.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.7.0+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (3.2.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3) (1.5.0)\n",
            "Requirement already satisfied: Pillow<=7.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3) (7.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3) (2018.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3) (0.8)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->stable-baselines3) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW1W5GOKFica"
      },
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
        "\n",
        "class SimpleCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    a simple callback\n",
        "\n",
        "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SimpleCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "        self.best_model_timestep = 0\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
        "                print(\"Best mean reward: {:.2f} (timestep {}) - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, self.best_model_timestep, mean_reward))\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  self.best_model_timestep = self.n_calls\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZf-k8AkNsKs",
        "outputId": "f9a9ae3a-6b9c-4123-97d6-4ecd84cf8798"
      },
      "source": [
        "! ls ./tmp/gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access './tmp/gym': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G02gXgMbdtNQ",
        "outputId": "a861a148-deb0-40b2-aeed-811530bd8fcd"
      },
      "source": [
        "# Read in Train_Sampels\n",
        "from csv import reader\n",
        "# read csv file as a list of lists\n",
        "with open('interaction_icaros/moderate_dataset/urgan_train_samples.csv', 'r') as read_obj:\n",
        "    # pass the file object to reader() to get the reader object\n",
        "    csv_reader = reader(read_obj)\n",
        "    # Pass reader object to list() to get a list of lists\n",
        "    list_of_rows = list(csv_reader)\n",
        "\n",
        "train_samples = [[float(j) for j in i] for i in list_of_rows]\n",
        "print(train_samples[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9.6747407913208, 8.644445419311523, 8.3929443359375, 7.823976516723633, 3.3333001136779785, 3.3333001136779785, 3.3333001136779785, 9.697210311889648, 10.32068157196045, 8.560725212097168, 8.241660118103027]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_je-uNZt0ag",
        "outputId": "8733bbb1-f880-44d4-dd17-546d53860ad6"
      },
      "source": [
        "# Read in Random_Sampels\n",
        "from csv import reader\n",
        "# read csv file as a list of lists\n",
        "with open('interaction_icaros/moderate_dataset/urgan_random_samples.csv', 'r') as read_obj:\n",
        "    # pass the file object to reader() to get the reader object\n",
        "    csv_reader = reader(read_obj)\n",
        "    # Pass reader object to list() to get a list of lists\n",
        "    list_of_rows = list(csv_reader)\n",
        "\n",
        "random_samples = [[float(j) for j in i] for i in list_of_rows]\n",
        "print(random_samples[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3.1084485054016113, 2.0046157836914062, 0.5570195913314819, 4.458590984344482, 3.3333001136779785, 3.3333001136779785, 3.3333001136779785, 2.597034215927124, 6.220783710479736, 0.9501394033432007, 4.877120018005371]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9awdnyBuT06",
        "outputId": "5451c1c8-8bed-4d6f-b8aa-ecc32fd09354"
      },
      "source": [
        "# Read in Gen_Sampels\n",
        "from csv import reader\n",
        "# read csv file as a list of lists\n",
        "with open('interaction_icaros/moderate_dataset/urgan_playable_samples_z-64.csv', 'r') as read_obj:\n",
        "    # pass the file object to reader() to get the reader object\n",
        "    csv_reader = reader(read_obj)\n",
        "    # Pass reader object to list() to get a list of lists\n",
        "    list_of_rows = list(csv_reader)\n",
        "\n",
        "gen_samples = [[float(j) for j in i] for i in list_of_rows]\n",
        "print(gen_samples[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8.725035667419434, 8.136852264404297, 3.223357915878296, 10.357789993286133, 3.3333001136779785, 3.3333001136779785, 3.3333001136779785, 3.8974783420562744, 7.564384937286377, 6.974689960479736, 6.13139009475708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqbNM_jgucDd",
        "outputId": "98325773-5694-48a4-c1ac-2f813fc4eb5e"
      },
      "source": [
        "# Read in Test_Sampels\n",
        "from csv import reader\n",
        "# read csv file as a list of lists\n",
        "with open('interaction_icaros/moderate_dataset/urgan_test_samples.csv', 'r') as read_obj:\n",
        "    # pass the file object to reader() to get the reader object\n",
        "    csv_reader = reader(read_obj)\n",
        "    # Pass reader object to list() to get a list of lists\n",
        "    list_of_rows = list(csv_reader)\n",
        "\n",
        "test_samples = [[float(j) for j in i] for i in list_of_rows]\n",
        "print(test_samples[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[11.03603458404541, 8.673916816711426, 9.168387413024902, 8.062556266784668, 3.3333001136779785, 3.3333001136779785, 3.3333001136779785, 6.057044982910156, 11.403951644897461, 12.378422737121582, 11.44744873046875]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42R5qRvrOi0J",
        "outputId": "df3d19a3-0b19-45b9-97e4-28d96f931bac"
      },
      "source": [
        "TRAIN_LEVELS_N = 10\n",
        "\n",
        "GEN_LEVELS_N = 40\n",
        "\n",
        "combined_list =  []\n",
        "\n",
        "for i in range(TRAIN_LEVELS_N):\n",
        "  combined_list.append(train_samples[i])\n",
        "\n",
        "for i in range(GEN_LEVELS_N):\n",
        "  combined_list.append(random_samples[i])\n",
        "\n",
        "print(combined_list[0])\n",
        "print(combined_list[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9.6747407913208, 8.644445419311523, 8.3929443359375, 7.823976516723633, 3.3333001136779785, 3.3333001136779785, 3.3333001136779785, 9.697210311889648, 10.32068157196045, 8.560725212097168, 8.241660118103027]\n",
            "[3.1084485054016113, 2.0046157836914062, 0.5570195913314819, 4.458590984344482, 3.3333001136779785, 3.3333001136779785, 3.3333001136779785, 2.597034215927124, 6.220783710479736, 0.9501394033432007, 4.877120018005371]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "272BWguFP9J3"
      },
      "source": [
        "### **Traing on Multiple Levels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnG7xxlf98QY",
        "outputId": "aa5ee3bd-2ee0-4d8d-94bb-66d966a2b8f4"
      },
      "source": [
        "import gym\n",
        "import os\n",
        "import cv2\n",
        "from stable_baselines3.common.monitor import Monitor as M\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3 import A2C\n",
        "from random import randint\n",
        "# Create log dir\n",
        "log_dir = \"./tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "# Logs will be saved in log_dir/monitor.csv\n",
        "# env = wrap_env(env, log_dir)\n",
        "\n",
        "\n",
        "env = M(env, log_dir)\n",
        "\n",
        "model = A2C('MlpPolicy',env, verbose=0)\n",
        "\n",
        "callback = SimpleCallback(check_freq=1000, log_dir=log_dir)\n",
        "\n",
        "\n",
        "#================ Training on Multiple levels =====================\n",
        "\n",
        "# The number of timestamps for training each level\n",
        "LEVEL_TIMESTEPS = 2000\n",
        "\n",
        "# The number of levels \n",
        "LEVEL_NUMS = 300\n",
        "\n",
        "# Total timestamps = ROUND_NUMS * LEVEL_TIMESTEPS * LEVEL_NUMS\n",
        "\n",
        "# in each round, we training on each one of the level in order with LEVEL_TIMESTEPS training time\n",
        "timestamp_counter = 0\n",
        "\n",
        "\n",
        "# CHANGE 1 ==================\n",
        "NUM_OF_LEVELS = len(combined_list)\n",
        "\n",
        "for j in range(LEVEL_NUMS):\n",
        "  num = randint(0, NUM_OF_LEVELS-1)\n",
        "  init_position = randint(1,18)\n",
        "  env.set_initial_x(init_position)\n",
        "  # CHANGE 2 ==================\n",
        "  env.load_terrain(combined_list[num])\n",
        "  model.learn(total_timesteps=LEVEL_TIMESTEPS, callback=callback)\n",
        "  timestamp_counter += LEVEL_TIMESTEPS\n",
        "  if timestamp_counter % 100000 == 0:\n",
        "    print(timestamp_counter)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_x 10.0\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -inf (timestep 0) - Last mean reward per episode: -246.42\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -246.42 (timestep 1000) - Last mean reward per episode: -298.38\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -246.42 (timestep 1000) - Last mean reward per episode: -322.43\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -246.42 (timestep 1000) - Last mean reward per episode: -296.87\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -246.42 (timestep 1000) - Last mean reward per episode: -293.19\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -246.42 (timestep 1000) - Last mean reward per episode: -264.46\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -246.42 (timestep 1000) - Last mean reward per episode: -262.55\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -246.42 (timestep 1000) - Last mean reward per episode: -256.80\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -246.42 (timestep 1000) - Last mean reward per episode: -243.44\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -243.44 (timestep 9000) - Last mean reward per episode: -235.85\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -235.85 (timestep 10000) - Last mean reward per episode: -225.96\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -225.96 (timestep 11000) - Last mean reward per episode: -221.95\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -221.95 (timestep 12000) - Last mean reward per episode: -227.57\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -221.95 (timestep 12000) - Last mean reward per episode: -223.31\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -221.95 (timestep 12000) - Last mean reward per episode: -217.32\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -217.32 (timestep 15000) - Last mean reward per episode: -208.08\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -208.08 (timestep 16000) - Last mean reward per episode: -193.92\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -193.92 (timestep 17000) - Last mean reward per episode: -193.14\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -193.14 (timestep 18000) - Last mean reward per episode: -192.39\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -192.39 (timestep 19000) - Last mean reward per episode: -189.26\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -189.26 (timestep 20000) - Last mean reward per episode: -185.92\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -185.92 (timestep 21000) - Last mean reward per episode: -181.10\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -181.10 (timestep 22000) - Last mean reward per episode: -178.54\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -178.54 (timestep 23000) - Last mean reward per episode: -167.64\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -167.64 (timestep 24000) - Last mean reward per episode: -157.00\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -157.00 (timestep 25000) - Last mean reward per episode: -142.29\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -142.29 (timestep 26000) - Last mean reward per episode: -131.89\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -131.89 (timestep 27000) - Last mean reward per episode: -125.67\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -125.67 (timestep 28000) - Last mean reward per episode: -117.19\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -117.19 (timestep 29000) - Last mean reward per episode: -111.66\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -111.66 (timestep 30000) - Last mean reward per episode: -106.85\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -106.85 (timestep 31000) - Last mean reward per episode: -107.48\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -106.85 (timestep 31000) - Last mean reward per episode: -95.82\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -95.82 (timestep 33000) - Last mean reward per episode: -84.54\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -84.54 (timestep 34000) - Last mean reward per episode: -72.88\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -72.88 (timestep 35000) - Last mean reward per episode: -64.24\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -64.24 (timestep 36000) - Last mean reward per episode: -67.26\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -64.24 (timestep 36000) - Last mean reward per episode: -69.66\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -64.24 (timestep 36000) - Last mean reward per episode: -66.57\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -64.24 (timestep 36000) - Last mean reward per episode: -59.67\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -59.67 (timestep 40000) - Last mean reward per episode: -54.93\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -54.93 (timestep 41000) - Last mean reward per episode: -49.09\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -49.09 (timestep 42000) - Last mean reward per episode: -41.83\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -41.83 (timestep 43000) - Last mean reward per episode: -36.15\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -36.15 (timestep 44000) - Last mean reward per episode: -29.37\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -29.37 (timestep 45000) - Last mean reward per episode: -29.30\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -29.30 (timestep 46000) - Last mean reward per episode: -29.31\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -29.30 (timestep 46000) - Last mean reward per episode: -27.12\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -27.12 (timestep 48000) - Last mean reward per episode: -24.03\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -24.03 (timestep 49000) - Last mean reward per episode: -24.58\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -24.03 (timestep 49000) - Last mean reward per episode: -26.00\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -24.03 (timestep 49000) - Last mean reward per episode: -16.79\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -16.79 (timestep 52000) - Last mean reward per episode: -11.10\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -11.18\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -18.89\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -16.97\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -24.55\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -23.98\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -24.80\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -28.82\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -30.00\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -31.24\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -29.88\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -30.51\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -32.36\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -34.47\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -41.57\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.79\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -44.88\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.76\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -51.98\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -60.68\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.37\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -67.81\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -69.95\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -76.58\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -77.60\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -87.65\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -88.25\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -86.09\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -80.80\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -80.43\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -79.93\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -74.87\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -76.42\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -72.57\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -73.78\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -65.69\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -64.98\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -65.56\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -70.52\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -68.26\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -60.99\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -50.97\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.76\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.61\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.71\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -36.55\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -33.20\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -27.66\n",
            "100000\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.14\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -67.84\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -71.70\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -70.51\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -62.20\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.24\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -47.55\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.63\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.63\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.69\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -43.84\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -38.54\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -47.13\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -50.97\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -44.05\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -37.67\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -37.41\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -41.39\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.43\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -43.00\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -43.09\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.54\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -44.59\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.82\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.32\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.73\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.85\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -51.20\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -52.62\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -50.72\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -52.36\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.15\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -56.77\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -57.86\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -56.74\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.49\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -60.07\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -62.23\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -63.99\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -65.79\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -69.22\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -71.97\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -73.08\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -74.58\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -74.18\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -73.37\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -68.97\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -68.47\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -68.57\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -68.08\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -69.35\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -68.99\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -69.02\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -69.23\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -65.09\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.28\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.68\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -59.52\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -60.82\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -67.23\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -69.45\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -67.99\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -63.13\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -67.54\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -80.95\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -81.85\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -83.97\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -89.39\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -99.75\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -102.31\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -99.76\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -100.01\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -97.81\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -94.76\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -90.20\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -79.87\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -68.54\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.53\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -51.77\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -43.25\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -39.36\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -41.12\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -39.25\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -35.44\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -38.17\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.36\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -47.20\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.73\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.48\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -50.21\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.29\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -57.51\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -57.95\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.40\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -59.21\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -60.84\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -62.93\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -59.92\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.68\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.36\n",
            "200000\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.14\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.54\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -56.33\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.64\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.15\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.24\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.50\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -50.57\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -51.99\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.63\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -53.32\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -53.84\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -55.39\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -60.26\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.90\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.20\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -56.03\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -53.70\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.30\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.53\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.79\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -56.24\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -53.78\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -53.20\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.07\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.90\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -56.02\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -54.19\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -51.43\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -52.88\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -53.14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -52.85\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.45\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.79\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -41.14\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -41.73\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -41.00\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.01\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -43.43\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -43.40\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -43.98\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -44.66\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.45\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.61\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -47.16\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.12\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.38\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.66\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.89\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.73\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -50.08\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -50.39\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -50.09\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.01\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.80\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -44.96\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.09\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.69\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -43.09\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.23\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -41.24\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -32.17\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -30.95\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -31.54\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -30.77\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -29.93\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -28.17\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -27.66\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -29.94\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -30.43\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -30.31\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -39.39\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -38.37\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.69\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.60\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.84\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.54\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.38\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -38.62\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -35.54\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -37.84\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -40.31\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -39.88\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.49\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -42.30\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -38.54\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -38.66\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -38.70\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -38.83\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -38.50\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.09\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.08\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.88\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -51.72\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -55.49\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -59.67\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -55.84\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -44.48\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.27\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -47.19\n",
            "300000\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -44.14\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -44.66\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -44.63\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.66\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.56\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.07\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -45.27\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -47.63\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.55\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -50.83\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -59.83\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -60.88\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.94\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.92\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -62.22\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.81\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.91\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.43\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.42\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.30\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -59.76\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -59.03\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -59.23\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.48\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -57.74\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.09\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.48\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -57.77\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -53.29\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -51.07\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -50.50\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -55.42\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -56.59\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -63.09\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -62.89\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -63.58\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -67.84\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -67.56\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -69.94\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -74.48\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -77.07\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -76.25\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -74.26\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -76.10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -77.46\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -76.79\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -80.09\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -81.75\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -84.61\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -84.43\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -89.31\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -94.04\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -99.03\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -102.92\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -103.36\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -104.09\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -109.90\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -113.52\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -115.81\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -116.71\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -117.00\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.81\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -113.63\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -114.40\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.88\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -113.58\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -114.15\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -114.97\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -117.89\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -118.12\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -118.47\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -123.84\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -124.72\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -125.94\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -125.57\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -124.12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -123.18\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -122.31\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -122.80\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -123.34\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -122.97\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -123.33\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -121.85\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -121.02\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -120.62\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -121.18\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -120.44\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -120.02\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -117.92\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -117.07\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -119.09\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -119.85\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -121.92\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -117.89\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -115.42\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.96\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.96\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -111.65\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -111.69\n",
            "400000\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -111.07\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -111.96\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -111.49\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.62\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -113.61\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.83\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.08\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.48\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -111.70\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -111.22\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.60\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.07\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -111.48\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -108.62\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.14\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -113.01\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -112.94\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -113.35\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -114.97\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -115.90\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -116.50\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -116.89\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -116.69\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -116.27\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -117.22\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -117.00\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -118.60\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -116.31\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -115.32\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -115.01\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -114.83\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -116.56\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -115.17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -115.02\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -114.69\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -109.81\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -110.11\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -113.19\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -110.04\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -109.99\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -105.97\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -99.77\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -98.08\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -90.14\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -84.91\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -81.02\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -79.80\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -75.78\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -75.59\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -76.63\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -74.05\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -71.70\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -70.11\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -69.20\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -64.97\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.74\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.89\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.79\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.23\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.36\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -61.42\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -60.26\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -60.35\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -60.66\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -59.38\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -59.67\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -56.74\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -56.51\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -55.30\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.32\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -58.45\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -56.49\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -55.44\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -55.38\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -53.44\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -53.68\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -55.64\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.93\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -57.02\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -51.00\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.08\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "initial_x 16\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -43.13\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -46.94\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.31\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -49.70\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -48.23\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -44.06\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -41.04\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -40.30\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -39.01\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -34.02\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -30.65\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -29.09\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -31.26\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -31.94\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -31.26\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -27.84\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -29.75\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -22.93\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -19.05\n",
            "500000\n",
            "initial_x 2\n",
            "initial_x 2\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -17.75\n",
            "initial_x 2\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -16.94\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -16.00\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -13.57\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -11.10 (timestep 53000) - Last mean reward per episode: -10.80\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 9\n",
            "initial_x 9\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -10.80 (timestep 505000) - Last mean reward per episode: -5.22\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -5.22 (timestep 506000) - Last mean reward per episode: -5.18\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -5.18 (timestep 507000) - Last mean reward per episode: -5.17\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -5.17 (timestep 508000) - Last mean reward per episode: -3.78\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -3.78 (timestep 509000) - Last mean reward per episode: -5.27\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -3.78 (timestep 509000) - Last mean reward per episode: -4.33\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -3.78 (timestep 509000) - Last mean reward per episode: -3.02\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -3.02 (timestep 512000) - Last mean reward per episode: 0.28\n",
            "Saving new best model to ./tmp/gym/best_model\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -2.28\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -1.56\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -4.87\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -7.35\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -9.96\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -10.01\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -5.38\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -2.03\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -0.66\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -8.06\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -6.73\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -4.30\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -4.34\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -6.06\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -0.86\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -1.26\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -0.09\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -10.81\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "initial_x 11\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -14.41\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -15.61\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -13.96\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -15.45\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -14.53\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -16.03\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -13.88\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -16.83\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "initial_x 3\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -24.07\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -22.40\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -21.99\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -23.67\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -21.80\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -28.31\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -30.19\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -30.27\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -24.98\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -26.87\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -32.22\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -33.11\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -31.56\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -33.53\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -38.40\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -35.16\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -35.18\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -34.50\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -30.87\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -31.36\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -27.14\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -25.68\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -29.61\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -24.72\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -26.88\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -22.54\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "initial_x 1\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -19.24\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -17.13\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -12.03\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -11.64\n",
            "initial_x 18\n",
            "initial_x 18\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -10.78\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -6.71\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -0.52\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: 0.19\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "initial_x 17\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -7.79\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -8.11\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -8.49\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -11.22\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "initial_x 12\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -17.29\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "initial_x 8\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -18.68\n",
            "initial_x 8\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -17.57\n",
            "initial_x 7\n",
            "initial_x 7\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -15.06\n",
            "initial_x 7\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -14.12\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -16.46\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "initial_x 10\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -16.65\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -12.83\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "initial_x 15\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -10.69\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -12.52\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -16.67\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -14.06\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "initial_x 5\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -13.74\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -8.56\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "initial_x 14\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -5.87\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -2.71\n",
            "initial_x 6\n",
            "initial_x 6\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -0.73\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -6.67\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "initial_x 13\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -13.63\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -13.58\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -16.90\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -22.04\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "initial_x 4\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 0.28 (timestep 513000) - Last mean reward per episode: -27.35\n",
            "600000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raxWQtQVQCeZ"
      },
      "source": [
        "### **Testing on Test Levels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJZLA0cXGP_A",
        "outputId": "7df96945-620c-4dd0-c7b8-d52340a03bfe"
      },
      "source": [
        "import gym\n",
        "import os\n",
        "import cv2\n",
        "from stable_baselines3.common.monitor import Monitor as M\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3 import A2C\n",
        "from random import randint\n",
        "\n",
        "model = A2C.load(\"./tmp/gym/best_model.zip\")\n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "TEST_LEVEL_NUMS = 20\n",
        "\n",
        "cumulated_reward_ls = []\n",
        "last_reward_ls = []\n",
        "\n",
        "for i in range(TEST_LEVEL_NUMS):\n",
        "  env.load_terrain(test_samples[i])\n",
        "  init_position = randint(1,18)\n",
        "  env.set_initial_x(init_position)\n",
        "  # Logs will be saved in log_dir/monitor.csv\n",
        "  log_dir='./video'\n",
        "  # env = wrap_env(env, log_dir)\n",
        "  obs = env.reset()\n",
        "  \n",
        "  # plt.imshow(env.render(mode='rgb_array'))\n",
        "  # plt.show()\n",
        "  # print(env.terrain_y_values)\n",
        "  cumulated_reward = 0.0\n",
        "  last_reward = 0.0\n",
        "  while True:\n",
        "      \n",
        "      action, _states = model.predict(obs, deterministic=True)\n",
        "      obs, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      last_reward = reward\n",
        "      im = env.render('rgb_array') \n",
        "      \n",
        "      if done: \n",
        "        break;\n",
        "  print(\"Cumulated Reward\", cumulated_reward, \"Last Reward\", last_reward)\n",
        "  cumulated_reward_ls.append(cumulated_reward)\n",
        "  last_reward_ls.append(last_reward)\n",
        "  plt.imshow(env.render(mode='rgb_array'))\n",
        "  plt.show()\n",
        "print(\" -------------- Final Average Results -----------------\")\n",
        "print(\"Mean Cumulated Reward\", sum(cumulated_reward_ls)/len(cumulated_reward_ls), \"Mean Last Reward\", sum(last_reward_ls)/len(last_reward_ls))\n",
        "              \n",
        "env.close()\n",
        "# show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_x 10.0\n",
            "initial_x 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "msuwfxPpRaqe",
        "outputId": "71619337-6f23-4231-8652-f888a8cf2405"
      },
      "source": [
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=50)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "\n",
        "    fig = plt.figure(title)\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed\")\n",
        "    plt.show()\n",
        "log_dir = \"./tmp/gym/\"\n",
        "plot_results(log_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHKk1jj9aTrV"
      },
      "source": [
        "!ls ./tmp/gym/best_model\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"./tmp/gym/best_model.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e9-bUKaa9Y9"
      },
      "source": [
        "import cv2\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "# Logs will be saved in log_dir/monitor.csv\n",
        "env = Monitor(env, log_dir)\n",
        "out = cv2.VideoWriter('project.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, (400,600))\n",
        " \n",
        "obs = env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    im = env.render('rgb_array')\n",
        "    out.write(im)\n",
        "    if done:\n",
        "      obs = env.reset()\n",
        "      print(reward)\n",
        "      plt.figure()\n",
        "      plt.imshow(im)\n",
        "\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBGMcRxxL1lZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}